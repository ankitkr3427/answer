
Na√Øve Bayes Classifier
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)
y_prob = gnb.predict_proba(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=iris.target_names)
auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')

print(f"Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", report)
print(f"\nAUC (Macro OVR): {auc:.4f}")

# Note: TP, TN, FP, FN are typically calculated per class from the confusion matrix.
# Example for class 0 (setosa) vs rest:
# TP = cm[0, 0]
# FN = cm[0, 1] + cm[0, 2]
# FP = cm[1, 0] + cm[2, 0]
# TN = cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2]
# Recall = TP / (TP + FN)
# Specificity = TN / (TN + FP)
# F1-Score = classification_report provides this per class.

OUTPUT:
Accuracy: 0.9778

Confusion Matrix:
 [[19  0  0]
 [ 0 12  1]
 [ 0  0 13]]

Classification Report:
               precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      0.92      0.96        13
   virginica       0.93      1.00      0.96        13

    accuracy                           0.98        45
   macro avg       0.98      0.97      0.97        45
weighted avg       0.98      0.98      0.98        45


AUC (Macro OVR): 1.0000

Simple Linear Regression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load data
diabetes = load_diabetes()
X = diabetes.data[:, np.newaxis, 2] # Using only the BMI feature
y = diabetes.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train model
slr = LinearRegression()
slr.fit(X_train, y_train)

# Predict
y_pred = slr.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"R-squared (R2) Score: {r2:.4f}")
print(f"Coefficient: {slr.coef_[0]:.4f}")
print(f"Intercept: {slr.intercept_:.4f}")

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, color='black', label='Actual data')
plt.plot(X_test, y_pred, color='blue', linewidth=3, label='Regression line')
plt.xlabel("BMI Feature")
plt.ylabel("Diabetes Progression")
plt.title("Simple Linear Regression Fit")
plt.legend()
plt.grid(True)
plt.show()

OUTPUT:
Mean Squared Error (MSE): 3884.9367
R-squared (R2) Score: 0.2803
Coefficient: 988.4193
Intercept: 151.0420


Multiple Linear Regression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load data
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train model
mlr = LinearRegression()
mlr.fit(X_train, y_train)

# Predict
y_pred = mlr.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"R-squared (R2) Score: {r2:.4f}")
print(f"Coefficients: {mlr.coef_}")
print(f"Intercept: {mlr.intercept_:.4f}")

# Plotting Predicted vs Actual
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, edgecolors='k')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2) # Diagonal line y=x
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Multiple Linear Regression: Actual vs. Predicted")
plt.grid(True)
plt.show()

OUTPUT:
Mean Squared Error (MSE): 2821.7510
R-squared (R2) Score: 0.4773
Coefficients: [  29.25401303 -261.7064691   546.29972304  388.39834056 -901.95966819
  506.76324136  121.15435079  288.03526689  659.26895081   41.37670105]
Intercept: 151.0082

Polynomial Regression
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score

np.random.seed(0)
X = 2 * np.random.rand(100, 1) - 1
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1) * 0.1

poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=42)

poly_reg = LinearRegression()
poly_reg.fit(X_train, y_train)
y_pred = poly_reg.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"R-squared (R2) Score: {r2:.4f}")
print(f"Coefficients: {poly_reg.coef_[0]}")
print(f"Intercept: {poly_reg.intercept_[0]:.4f}")

OUTPUT:
Mean Squared Error (MSE): 0.0075
R-squared (R2) Score: 0.9713
Coefficients: [0.9996097  0.46083222]
Intercept: 2.0342

5. Lasso and Ridge Regression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Load data
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Scale data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Lasso Regression
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)
mse_lasso = mean_squared_error(y_test, y_pred_lasso)
r2_lasso = r2_score(y_test, y_pred_lasso)

print("--- Lasso Regression ---")
print(f"Mean Squared Error (MSE): {mse_lasso:.4f}")
print(f"R-squared (R2) Score: {r2_lasso:.4f}")
print(f"Coefficients: {lasso.coef_}")
print(f"Intercept: {lasso.intercept_:.4f}")

# Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

print("\n--- Ridge Regression ---")
print(f"Mean Squared Error (MSE): {mse_ridge:.4f}")
print(f"R-squared (R2) Score: {r2_ridge:.4f}")
print(f"Coefficients: {ridge.coef_}")
print(f"Intercept: {ridge.intercept_:.4f}")

# Plotting Predicted vs Actual for Ridge
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_ridge, alpha=0.7, edgecolors='k')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2) # Diagonal line y=x
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values (Ridge)")
plt.title("Ridge Regression: Actual vs. Predicted")
plt.grid(True)
plt.show()
OUTPUT
--- Lasso Regression ---
Mean Squared Error (MSE): 2817.1157
R-squared (R2) Score: 0.4781
Coefficients: [  1.40076786 -12.21082056  26.22283624  18.27241085 -30.30252121
  14.47985761   0.          11.52188797  26.61636217   2.04485504]
Intercept: 150.9870

--- Ridge Regression ---
Mean Squared Error (MSE): 2820.0244
R-squared (R2) Score: 0.4776
Coefficients: [  1.48684207 -12.34118301  26.13559736  18.33026077 -32.94306736
  16.37517667   1.34406271  12.27111459  27.46151363   2.11494554]
Intercept: 150.9872



6. Logistic Regression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler

# Load data (binary classification)
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# Scale data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Train model
log_reg = LogisticRegression(solver='liblinear', random_state=42)
log_reg.fit(X_train, y_train)

# Predict
y_pred = log_reg.predict(X_test)
y_prob = log_reg.predict_proba(X_test)[:, 1] # Probability for the positive class

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=cancer.target_names)
auc = roc_auc_score(y_test, y_prob)
fpr, tpr, thresholds = roc_curve(y_test, y_prob)

tn, fp, fn, tp = cm.ravel()
error_rate = (fp + fn) / (tn + fp + fn + tp)
recall = tp / (tp + fn)
specificity = tn / (tn + fp)
# F1-score is in the report

print(f"Accuracy: {accuracy:.4f}")
print(f"Error Rate: {error_rate:.4f}")
print("\nConfusion Matrix:\n", cm)
print(f"\nTrue Positives (TP): {tp}")
print(f"True Negatives (TN): {tn}")
print(f"False Positives (FP): {fp}")
print(f"False Negatives (FN): {fn}")
print(f"\nRecall (Sensitivity): {recall:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"AUC: {auc:.4f}")
print("\nClassification Report:\n", report)

# Plotting ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Chance level')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()
OUTPUT:
Accuracy: 0.9825
Error Rate: 0.0175

Confusion Matrix:
 [[ 62   1]
 [  2 106]]

True Positives (TP): 106
True Negatives (TN): 62
False Positives (FP): 1
False Negatives (FN): 2

Recall (Sensitivity): 0.9815
Specificity: 0.9841
F1-Score (manual calc example): 0.9820
AUC: 0.9981

Classification Report:
               precision    recall  f1-score   support

   malignant       0.97      0.98      0.98        63
      benign       0.99      0.98      0.99       108

    accuracy                           0.98       171
   macro avg       0.98      0.98      0.98       171
weighted avg       0.98      0.98      0.98       171



7. Artificial Neural Network (Classification)
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler

cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42, alpha=0.0001, solver='adam')
mlp.fit(X_train, y_train)
y_pred = mlp.predict(X_test)
y_prob = mlp.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=cancer.target_names)
auc = roc_auc_score(y_test, y_prob)

tn, fp, fn, tp = cm.ravel()
error_rate = (fp + fn) / (tn + fp + fn + tp)
recall = tp / (tp + fn)
specificity = tn / (tn + fp)


print(f"Accuracy: {accuracy:.4f}")
print(f"Error Rate: {error_rate:.4f}")
print("\nConfusion Matrix:\n", cm)
print(f"\nTrue Positives (TP): {tp}")
print(f"True Negatives (TN): {tn}")
print(f"False Positives (FP): {fp}")
print(f"False Negatives (FN): {fn}")
print(f"\nRecall (Sensitivity): {recall:.4f}")
print(f"Specificity: {specificity:.4f}")
# F1-Score is available in the classification report
print(f"AUC: {auc:.4f}")
print("\nClassification Report:\n", report)

OUTPUT:
Accuracy: 0.9825
Error Rate: 0.0175

Confusion Matrix:
 [[ 61   2]
 [  1 107]]

True Positives (TP): 107
True Negatives (TN): 61
False Positives (FP): 2
False Negatives (FN): 1

Recall (Sensitivity): 0.9907
Specificity: 0.9683
AUC: 0.9972

Classification Report:
               precision    recall  f1-score   support

   malignant       0.98      0.97      0.98        63
      benign       0.98      0.99      0.99       108

    accuracy                           0.98       171
   macro avg       0.98      0.98      0.98       171
weighted avg       0.98      0.98      0.98       171

8. K-NN Classifier
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler

iris = load_iris()
X, y = iris.data, iris.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
y_prob = knn.predict_proba(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=iris.target_names)
auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')

print(f"Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", report)
print(f"\nAUC (Macro OVR): {auc:.4f}")
OUTPUT:
Accuracy: 1.0000

Confusion Matrix:
 [[19  0  0]
 [ 0 13  0]
 [ 0  0 13]]

Classification Report:
               precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      1.00      1.00        13
   virginica       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45


AUC (Macro OVR): 1.0000

9. Decision Tree Classification
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

dtree = DecisionTreeClassifier(random_state=42)
dtree.fit(X_train, y_train)
y_pred = dtree.predict(X_test)
y_prob = dtree.predict_proba(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=iris.target_names)
auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')

print(f"Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", report)
print(f"\nAUC (Macro OVR): {auc:.4f}")

OUTPUT:
Accuracy: 1.0000

Confusion Matrix:
 [[19  0  0]
 [ 0 13  0]
 [ 0  0 13]]

Classification Report:
               precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      1.00      1.00        13
   virginica       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45


AUC (Macro OVR): 1.0000

10. SVM Classification
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler, label_binarize
from itertools import cycle

# Load data
iris = load_iris()
X, y = iris.data, iris.target

# Scale data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Binarize the output for multi-class ROC (One-vs-Rest)
y_bin = label_binarize(y, classes=[0, 1, 2])
n_classes = y_bin.shape[1]

# Split data
X_train, X_test, y_train, y_test_bin = train_test_split(X_scaled, y_bin, test_size=0.3, random_state=42)
_, _, y_train_orig, y_test_orig = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Keep original labels for report

# Train model
# Using probability=True for predict_proba, needed for ROC
svm_clf = SVC(kernel='linear', probability=True, random_state=42)
# Fit on original multi-class labels
svm_clf.fit(X_train, y_train_orig)
y_pred = svm_clf.predict(X_test)
y_prob = svm_clf.predict_proba(X_test)

# Evaluate (using original multi-class labels for standard metrics)
accuracy = accuracy_score(y_test_orig, y_pred)
cm = confusion_matrix(y_test_orig, y_pred)
report = classification_report(y_test_orig, y_pred, target_names=iris.target_names)

# Calculate AUC (Macro OVR)
auc_macro = roc_auc_score(y_test_bin, y_prob, multi_class='ovr', average='macro')

print(f"Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", report)
print(f"\nAUC (Macro OVR): {auc_macro:.4f}")

# Plotting ROC Curve for each class (One-vs-Rest)
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
    roc_auc[i] = roc_auc_score(y_test_bin[:, i], y_prob[:, i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())
roc_auc["micro"] = roc_auc_score(y_test_bin, y_prob, multi_class='ovr', average='micro')


plt.figure(figsize=(8, 6))
colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(iris.target_names[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance level')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) - SVM (One-vs-Rest)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()
OUTPUT:
Accuracy: 0.9778

Confusion Matrix:
 [[19  0  0]
 [ 0 12  1]
 [ 0  0 13]]

Classification Report:
               precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      0.92      0.96        13
   virginica       0.93      1.00      0.96        13

    accuracy                           0.98        45
   macro avg       0.98      0.97      0.97        45
weighted avg       0.98      0.98      0.98        45


AUC (Macro OVR): 1.0000


11. K-Means Clustering
import numpy as np
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

iris = load_iris()
X = iris.data

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
kmeans.fit(X_scaled)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

silhouette_avg = silhouette_score(X_scaled, labels)

print(f"Number of clusters: 3")
print(f"Cluster Centroids:\n{centroids}")
print(f"\nCluster Labels for first 10 samples:\n{labels[:10]}")
print(f"\nSilhouette Score: {silhouette_avg:.4f}")

OUTPUT:
Number of clusters: 3
Cluster Centroids:
[[-0.05021989 -0.88337647  0.34773781  0.2815273 ]
 [-1.01457897  0.85326268 -1
